# 为何需要爬虫框架

接着来解释，为何要用爬虫框架：

* 框架帮你把大部分重复的工作都实现了
  * 做了哪些通用的事情
    * `下载`
      * 网络异常时`自动重试`retry
        * 还可以设置
          * 最大`重试次数`：最多重试几次
            * 如果还是不行，才视为下载失败
          * `重试间隔`：两次重试之间的间隔时间
        * 好处：不用担心偶尔某次网络有问题，就导致下载失败了，因为还可以自动重试
        * 对比：自己裸写代码，就要考虑这种异常情况，导致自己爬虫代码臃肿和逻辑复杂
          * 花了太多精力在和爬取数据关系不大的方面，不值得，效率低
      * 下载`进程的管理`
        * 同时发出多个url请求去下载内容
        * 有专门的进程管理和调度策略
        * 好处：能同时并发多个请求
        * 对比：自己裸写代码去下载，往往同一时刻只能有一个请求
          * 否则就要花很多精力去实现并发
      * `url去重`
        * 前后（不同页面，不同场景下）发出的多个url中是否有重复的
          * 如果有，则自动忽略掉，去掉，去除重复=去重
    * `提取`
      * 做了啥
        * 内置常用的内容提取的库
          * `PySpider`集成`PyQuery`
          * `Scrapy`集成选择器，支持：`xpath`、`css`、`re`
        * 同时支持可选的第三方的库
          * `Scrapy`也支持用`BeautifulSoup`提取内容
      * 好处：不用额外安装和使用这些库
      * 对比：自己裸写代码就要考虑选用哪些合适的库去提取内容
    * 保存
      * 做了啥
        * 集成各种保存数据的接口和框架
          * `PySpider`
            * 自带默认保存为`sqlite`中
              * 可以从界面中导出`csv`或`json`
            * 其他数据库接口
              * `mysql`
              * `mongodb`
              * 等等
      * 好处：可以方便的选择保存数据的方式，无需过多操心细节
      * 对比：自己裸写代码，还要安装不同的数据库的库，再手动写（sql等）代码去保存数据
* 还带了很多额外的好用功能
  * `PySpider`
    * 带`UI界面`，`调试非常方便`
    * 支持网页内容是`执行js`后才生成的
      * 通过集成第三方`phantomjs`

